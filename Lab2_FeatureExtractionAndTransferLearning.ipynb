{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdTAiE71iHF3"
      },
      "source": [
        "# Feature extraction and transfer learning with pre-trained CNNs\n",
        "Before proceeding *REMEMBER TO ENABLE GPU IN THE RUNTIME ENVIRONMENT:* Go to Runtime -> \"Change runtime type\" and select GPU as hardware acelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tmk836r9sK"
      },
      "source": [
        "## Task 1: Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZn30wetMkxe"
      },
      "source": [
        "In this lab we'll be using a small dataset containing images of cats, dogs, and horses.\n",
        "\n",
        "To download the dataset, run the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc7woIJ4OWNr"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# File ID\n",
        "file_id = \"1KDMC39ba1MAL83FLLoSVSJY2KOmFR1aj\"\n",
        "output = \"data.zip\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJhwhzY2O7GJ"
      },
      "source": [
        "Now, unzip the data file and show the contents of the `data` folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtuhIYewPItF"
      },
      "outputs": [],
      "source": [
        "!unzip -q data.zip\n",
        "!ls data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIF09v0JPmgv"
      },
      "source": [
        "Verify that the data folder contains a subfolder for each of the classes (i.e., cat, dog, and horse).\n",
        "\n",
        "Let's display one of the cat images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcC5MZJFPtzK"
      },
      "outputs": [],
      "source": [
        "download_url(urls[0], 'cat.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAqrkRVVP0dZ"
      },
      "source": [
        "Let's display it also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQuJMvwoP3HS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = cv2.imread(\"./data/cat/05069002-c34e-4d79-adaa-11760eb613ee.jpg\")\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); # Recall OpenCV reads images as BGR\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2wNWh4zOiz"
      },
      "source": [
        "## Task 2 (optional): Mount your Google Drive and upload the data file\n",
        "Save this notebook to your Google Drive by selecting \"Save\" or \"Save a copy in Drive\" in the Files menu.\n",
        "\n",
        "If you want to store data permanently, you also need to mount your Google Drive, which can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYezthubzNRt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJjrzCdf0WSe"
      },
      "source": [
        "Your Google Drive is now mounted in `/content/gdrive/MyDrive`, and you can use normal shell commands to create directories and copy files to/from Google Drive.\n",
        "\n",
        "For instance, to copy the data file to the root of your Google Drive, you can run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH00yY6d0dWi"
      },
      "outputs": [],
      "source": [
        "!cp data.zip /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih6YdL9UieTV"
      },
      "source": [
        "And similarly, to copy the data file from your Google Drive to your Colab notebook environment, you can run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqV0bIIuiUPP"
      },
      "outputs": [],
      "source": [
        "!cp /content/gdrive/MyDrive/data.zip ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ZGqyqR2HS2"
      },
      "source": [
        "## Task 3: Set up neural network for feature extraction\n",
        "We will be using a deep learning framework, called [Keras](https://keras.io/). Keras is a high-level neural network API, written in Python and capable of running on top of [TensorFlow](https://www.tensorflow.org/), CNTK, and Theano.\n",
        "\n",
        "A common and highly effective approach to deep learning on small image datasets is to leverage a pre-trained network. A pre-trained network is simply a saved network previously trained on a large dataset, such as the [ImageNet dataset](http://www.image-net.org/) (1.4 million labeled images and 1000 different classes). If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task.\n",
        "\n",
        "In our case, we will consider a convolutional neural network (CNN) trained on ImageNet. We will use the MobileNet architecture, but there are other models that you could use as well. Take a look here: https://keras.io/applications\n",
        "\n",
        "There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. We will be covering both of them today. Let's start with feature extraction.\n",
        "\n",
        "Feature extraction consists of using the representations learned by an existing neural network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. This could be any classifier, such as K-Nearest Neighbours (KNN).\n",
        "\n",
        "Traditional CNNs are divided into two parts: they start with a series of convolution and pooling layers, and they end with a densely-connected classifier. The first part is often referred to as the \"encoder\", \"feature extractor\" or \"convolutional base\" of the model. In the case of CNNs, \"feature extraction\" will simply consist of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output. The second part of the network, called the \"decoder\" or \"top layers\", is ignored for now. We will be using it for fine-tuning (Transfer Learning) in Task 10.\n",
        "\n",
        "First, let's download and instantiate the pre-trained MobileNet without the top layers (i.e., without the decoder).\n",
        "\n",
        "You can ignore the warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxeBWYX76iCI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "\n",
        "conv_base = MobileNet(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(120, 120, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn92MTLj62fD"
      },
      "source": [
        "Let's summarize the model used for feature extraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxrNI8bk69Vj"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqOu5xum3T-5"
      },
      "source": [
        "That's a lot to digest if you haven't seen a convolutional neural network before. By the end of the course, you will know what it all means. For now, see if you can figure out the answers to these questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm_z3RAZ7DBI"
      },
      "source": [
        "### Questions 3.1\n",
        "1. What is the expected shape of the input image?\n",
        "2. What is the shape of the output of the model?\n",
        "3. What happens to the output shape if you double the size of the input image?\n",
        "4. Can you guess what the None dimension is used for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSNV10j_8AFc"
      },
      "source": [
        "## Task 4: Preprocessing\n",
        "Many neural networks expect the input image to have a fixed, pre-defined shape. Also, the pixel intensities are assumed to be in a fixed range.\n",
        "\n",
        "For reasons that will become clear later in the course, neural networks do not work well on images in which the intensities lie in the standard range from 0 to 255. Instead, we want the intensities to be centered around zero. Typical intensity ranges are -127.5 to 127.5 or -1 to 1, but it depends on the chosen neural network architecture. Each pre-trained network in Keras comes with its own *preprocessor*, which assures that the intensities are scaled correctly for that particular network.\n",
        "\n",
        "In summary, *preprocessing* refers to the step of preparing the image to be fed into the network by making sure the image has the right shape, and that the intensities are in the correct range.\n",
        "\n",
        "Let's load an image, preprocess it, and feed it through the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A79c9Ozj8yCV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras.utils as image\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "classes = ['cat','dog','horse']\n",
        "\n",
        "# Pick first image of first class (i.e., cat)\n",
        "filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n",
        "img_path = filelist[1]\n",
        "print(f\"File path: {img_path}\")\n",
        "\n",
        "# Load image and preprocess it\n",
        "img = image.load_img(img_path, target_size=(120, 120))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0)\n",
        "img_preprocessed = preprocess_input(img_data.copy())\n",
        "\n",
        "# Feed preprocessed image through CNN encoder to get a new feature representation\n",
        "mobilenet_features = conv_base.predict(img_preprocessed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZR30bOf-MIS"
      },
      "source": [
        "### Questions 4.1\n",
        "1. What is the range of the pixel values before and after preprocessing?\n",
        "2. So what formula do you think is used to pre-process the pixel values?\n",
        "3. What is the order of the color channels? (you could compare with ``cv2.imread``, which we know reads images as BGR)\n",
        "4. What is the size of the input image (``img_data`` after calling ``np.expand_dims``)?\n",
        "5. What is the size of the calculated feature representation (`mobilenet_features`)?\n",
        "6. So what is the reduction in dimensionality after feature extraction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk05mlKTr2U5"
      },
      "source": [
        "## Task 5: Feature maps\n",
        "As we have seen above, a convolutional neural network consists of many layers. Each layer performs some mathematical operation on the output of the previous layer. The operations have names like ``Conv2D`` , ``BatchNormalization``, ``ReLU``, and ``DepthwiseConv2D``, which you will learn about during the course.\n",
        "\n",
        "The output of a layer is referred to as a **feature map**.\n",
        "\n",
        "Let's look at some feature maps produced by the first layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeGt4WcVvjz_"
      },
      "outputs": [],
      "source": [
        "from keras import Model\n",
        "\n",
        "def show_feature_maps_from_layer(layer_name='conv1',img=img_preprocessed):\n",
        "  dummy_model = Model(inputs=conv_base.input, outputs=conv_base.get_layer(layer_name).output)\n",
        "  out = (dummy_model.predict(img)).squeeze()\n",
        "  height = out.shape[0]\n",
        "  width = out.shape[1]\n",
        "  num_channels = out.shape[2]\n",
        "  print(f'Feature map size: {height}x{width}x{num_channels}')\n",
        "\n",
        "  plt.figure(figsize=(16,16))\n",
        "  for i in range(8): # only display first 8 feature maps (channels)\n",
        "    f = out[:,:,i]\n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.imshow(f,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"{0:.2f}\".format(f.min()) + \"/\" + \"{0:.2f}\".format(f.max()))\n",
        "\n",
        "# See conv_base.summary() for complete list of layer names\n",
        "show_feature_maps_from_layer(layer_name='conv1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blR0tq57Ba22"
      },
      "source": [
        "Because this layer (``conv1``) is a convolution layer, each feature map results from applying a filter to the input image. As you can see, different filters highlight different features of the input image. The next layer in the network then takes these feature maps and transforms them somehow to extract new and more abstract features. In one of the later layers (``conv_dw_2``) we can still sort-of see the cat in the feature maps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFmjLmkZAsBE"
      },
      "outputs": [],
      "source": [
        "show_feature_maps_from_layer(layer_name='conv_dw_2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwv2iw6C3kD"
      },
      "source": [
        "In the last feature map it is impossible for us to see that there was originally a cat in the input image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zve01efLDD1p"
      },
      "outputs": [],
      "source": [
        "show_feature_maps_from_layer(layer_name='conv_pw_13_relu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrvSiAmIDRgy"
      },
      "source": [
        "### Question 5.1\n",
        "1. Notice that the height and width of the feature maps become smaller and smaller as we move deeper into the network. Why do you think that is?\n",
        "2. If the last feature map is only three pixels high and three pixels wide (shape is ``3x3x1024``), how can the neural network know that there is a cat in the image, i.e, where is that information encoded in the feature map?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIjrfD4ecsOS"
      },
      "source": [
        "##Task 6: How to use the image generator\n",
        "Loading and preprocessing images is such a common task in deep learning that frameworks like Keras provide predefined tools for us that we can use.\n",
        "In this task we will look at Keras' image data generator: https://keras.io/preprocessing/image/#imagedatagenerator-class. Simply put, the image generator is a tool that makes loading and preprocessing data easy.\n",
        "\n",
        "Let's set up an image generator that outputs mini-batches of 8 images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP9tCD3Je5WP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "generator = datagen.flow_from_directory(str(p), # this is where you specify the path to the main data folder\n",
        "                                        target_size=(120,120),\n",
        "                                        color_mode='rgb',\n",
        "                                        batch_size=8,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sya17-HqWD1z"
      },
      "source": [
        "**Note:** Check that the classes assigned by the generator are consistent with your class assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpd9vZt6WNrs"
      },
      "outputs": [],
      "source": [
        "print('generator:',generator.class_indices)\n",
        "print('your\\'s:',dict((class_name,class_index) for class_index,class_name in enumerate(classes)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4FSIy_fg5Q"
      },
      "source": [
        "Here is one way to generate a new batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC9X6GshfLYv"
      },
      "outputs": [],
      "source": [
        "inputs, labels = generator.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P535xuM7e9jL"
      },
      "source": [
        "### Questions 6.1\n",
        "1. What is variable ``inputs``? (Hint: look at the shape)\n",
        "2. What is variable ``labels``?\n",
        "3. How does the image generator know where the images are stored?\n",
        "4. How does the image generator know the class of each image?\n",
        "5. What does shuffle mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hQXxhFgLaJ"
      },
      "source": [
        "As you learned in lecture 2 it is always a good idea to split the data into a training set and a validation set. Again, this is such a common task in deep learning that the image generator can do it for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXA-IppWgYsi"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(str(p),\n",
        "                                        target_size=(120,120),\n",
        "                                        color_mode='rgb',\n",
        "                                        batch_size=8,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=True,\n",
        "                                        subset='training')\n",
        "validation_generator = datagen.flow_from_directory(str(p),\n",
        "                                        target_size=(120,120),\n",
        "                                        color_mode='rgb',\n",
        "                                        batch_size=8,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=True,\n",
        "                                        subset='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLpdkt3ehCI0"
      },
      "source": [
        "### Questions 6.2\n",
        "1. How does each of the two generators know if it should produce training or validation images?\n",
        "2. What is the validation percentage in this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yfcUFNAhSBS"
      },
      "source": [
        "## Task 7: Feature extraction\n",
        "Our goal is to train a machine learning model to correctly classify images belonging to the chosen categories (cat, dog, horse). Today, we will be using KNN, which operates on vectors. Therefore, we have to convert our images into vectors.\n",
        "\n",
        "We have two choices of features:\n",
        "\n",
        "1. Raw pixel values\n",
        "2. MobileNet features\n",
        "\n",
        "Besides the features we will of course also need the labels (cat, dog, horse).\n",
        "\n",
        "Let's create the datasets that we will be using:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwc0rObZJ05v"
      },
      "outputs": [],
      "source": [
        "def extract_features(generator):\n",
        "  generator.reset()\n",
        "  raw_pixel_features_list = []\n",
        "  mobilenet_features_list = []\n",
        "  labels_list = []\n",
        "  batch_index = 0\n",
        "  while batch_index < generator.num_batches:\n",
        "    # Load mini-batch\n",
        "    raw_pixels, labels = generator.__getitem__(batch_index)\n",
        "\n",
        "    # Run through MobileNet encoder\n",
        "    mobilenet_features = conv_base.predict(raw_pixels) # This is where we apply the CNN\n",
        "\n",
        "    # Vectorize images\n",
        "    bs,h,w,c = raw_pixels.shape\n",
        "    raw_pixels = np.reshape(raw_pixels,(bs,h*w*c)) # vectorize\n",
        "\n",
        "    # Vectorize MobileNet features\n",
        "    bs,h,w,c = mobilenet_features.shape\n",
        "    mobilenet_features = np.reshape(mobilenet_features,(bs,h*w*c)) # vectorize\n",
        "\n",
        "    # Convert one-hot encoding to class index\n",
        "    labels = np.argmax(labels,axis=1)\n",
        "\n",
        "    # Save in lists\n",
        "    for i in range(bs):\n",
        "      raw_pixel_features_list.append(raw_pixels[i])\n",
        "      mobilenet_features_list.append(mobilenet_features[i])\n",
        "      labels_list.append(labels[i])\n",
        "\n",
        "    batch_index = batch_index + 1\n",
        "\n",
        "  # Convert lists to numpy arrays\n",
        "  raw_pixel_features = np.asarray(raw_pixel_features_list)\n",
        "  mobilenet_features = np.asarray(mobilenet_features_list)\n",
        "  labels = np.asarray(labels_list)\n",
        "\n",
        "  return raw_pixel_features, mobilenet_features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S9FZc68heuV"
      },
      "outputs": [],
      "source": [
        "train_features_raw, train_features_mobilenet, train_labels = extract_features(train_generator)\n",
        "validation_features_raw, validation_features_mobilenet, validation_labels = extract_features(validation_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEx-I6B20U0P"
      },
      "source": [
        "Show the first 32 images of the validation data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plz9liatrfhW"
      },
      "outputs": [],
      "source": [
        "def denormalize(input_img):\n",
        "  '''\n",
        "    input_img has intensities in range -1 to 1 (after Keras MobileNet preprocessing)\n",
        "    output_img has intensities in range 0 to 1 (float)\n",
        "  '''\n",
        "  output_img = (input_img+1) / 2\n",
        "  return output_img\n",
        "\n",
        "def vec2img(img_as_vec,output_shape=(120,120,3)):\n",
        "  '''\n",
        "    img_as_vec is a vectorized color image\n",
        "    output_shape is the desired output image shape\n",
        "  '''\n",
        "  img_as_array = np.reshape(img_as_vec,output_shape)\n",
        "  return img_as_array\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "for i in range(32):\n",
        "  # reshape feature vector into 120x120x3 array (image) and de-normalize intensities to range 0 to 1.\n",
        "  img = vec2img(validation_features_raw[i,:])\n",
        "  img = denormalize(img)\n",
        "  plt.subplot(4,8,i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([]), plt.yticks([])\n",
        "  plt.title(classes[int(validation_labels[i])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AAEN6DRUIKu"
      },
      "source": [
        "**Sub-task:** Verify that the labels above are correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r64k-mEESwzV"
      },
      "source": [
        "### Questions 7.1\n",
        "The image data has now been vectorized.\n",
        "1. What is the shape of ``train_features_raw``?\n",
        "2. What is the shape of ``train_features_mobilenet``?\n",
        "3. What is the difference between ``train_features_raw`` and ``train_features_mobilenet``, i.e., what do they represent?\n",
        "4. What is the shape of ``train_labels``?\n",
        "5. How many training samples do we have, and how many validation samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsyddx2zjdlL"
      },
      "source": [
        "##Task 8: Classify images using K-Nearest Neighbours (K-NN) classifier\n",
        "Your task is to train a K-NN classifier on the training set, and evaluate the performace on the validation set by calculating the accuracy (and remember to calculate the accuracy on the validation set, not the training set!).\n",
        "\n",
        "First solve the task using the raw pixels as features:\n",
        "\n",
        "```\n",
        "# Training set\n",
        "train_features_raw, train_labels\n",
        "\n",
        "# Validation set\n",
        "validation_features_raw, validation_labels\n",
        "```\n",
        "\n",
        "Then solve the same task using the MobileNet features:\n",
        "\n",
        "```\n",
        "# Training set\n",
        "train_features_mobilenet, train_labels\n",
        "\n",
        "# Validation set\n",
        "validation_features_mobilenet, validation_labels\n",
        "```\n",
        "\n",
        "Compare the results and explain the difference.\n",
        "\n",
        "You are on your own here, but **you don't have to implement K-NN yourself**. I suggest you use [scikit-learn](https://scikit-learn.org) (and Google)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KlBveFjV8U5"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN39UXf8syYV"
      },
      "source": [
        "**Comparison:** If everything went as planned, you should be able to conclude that the accuracy of the K-NN classifier is significantly higher when using the neural networks features compared to when using the raw pixel values as features. This is becuase the MobileNet has already been pre-trained, i.e., it has learned features that are useful for classifying 1000 different object categories. In almost any scenario you can think of, MobileNet's feature representation will be better than using the raw pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDMVkUF2JHrx"
      },
      "source": [
        "### Comments\n",
        "So what we have learned so far is that images of the same class tend to group closer together when using MobileNet's feature representation, but not so much when using the raw intensities. This confirms that using the raw pixels as features is in general not a good idea.\n",
        "\n",
        "The reason that MobileNet's feature representation works better is because the network has learned to map images onto a manifold. A manifold is kind of like a low-dimensional surface that exists in a high-dimensional space. For instance if images of faces were to be mapped into a 4D manifold, the first axis on the manifold could represent gender, and the others could represent age, view angle, and eye color. You can read more about manifold learning in chapter 5.11.3 of the book.\n",
        "\n",
        "The underlying hypothesis of using K-NN to classifiy images using MobileNet features is that objects that are similar will map to the approximate same location on some manifold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYeEks0j_Tky"
      },
      "source": [
        "## Task 9: Transfer learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfOBC8RNXByP"
      },
      "source": [
        "Putting your own K-NN classifier on top of a pre-trained CNN is not really optimal. Why? Because, while the features of the convolutional base are better than using raw pixel values, they are not guaranteed to 100% optimal for your specific task. So, a better solution is to attach a second neural network on top of the convolutional base, and train both the classifier *and* the convolutional base at the same time. This is called **transfer learning**. The extra neural network that is put on top of our encoder is often referred to as a \"decoder\" or a \"classification head\".\n",
        "\n",
        "Recall that CNNs like AlexNet and MobileNet have been trained on ImageNet, which contains 1000 classes. If you download Keras' pre-trained models *including the top layers* (i.e., the decoder), the top layers are in fact the classifier that we want to replace. Let's verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj1lyzlHXND0"
      },
      "outputs": [],
      "source": [
        "mobilenet_full = MobileNet(weights='imagenet',\n",
        "                      include_top=True,\n",
        "                      input_shape=(224, 224, 3))\n",
        "mobilenet_full.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrTizMfWXmyX"
      },
      "source": [
        "### Questions 9.1\n",
        "Inspect the printout above.\n",
        "1.  Can you identify the convolutional base of this network? (Compare to the ```conv_base``` model we used earlier.)\n",
        "2. All layers beyond the convolutional base represent the classifier (or decoder). How many classes are there?\n",
        "3. So what is the size of the output of the model?\n",
        "4. Can you guess how we should interpret the output of model?\n",
        "5. The input size must be 224 by 224 pixels (you can verify for yourself). Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRtmTVrn2U8v"
      },
      "source": [
        "So, how do we modify and re-train MobileNet to work on our own data? First of all, we don't want to train CNNs from scratch, since this could take days. Secondly, we need to modify the network architecture to output three class labels (cat, dog, horse) instead of 1000.\n",
        "\n",
        "The main hypothesis underlying transfer learning is that the network weights learned in the convolutional layers (i.e., the *encoder*) are generic and need little or no fine-tuning to work on other data sets or tasks. So in practice, we just need to replace and re-train the last layers (i.e., the *decoder*) of a pre-trained network.\n",
        "\n",
        "So let's take our convolutional base (encoder) and put a simple neural network classifier (decoder) on top of it. Your task is to figure out what the value of variable N should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAtJ3XgK5tTG"
      },
      "outputs": [],
      "source": [
        "N = ???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1OMmP__03MB"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "# Add new top layer\n",
        "x = conv_base.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024,activation='relu')(x) #dense layer\n",
        "preds = Dense(N,activation='softmax')(x) #final layer with softmax activation\n",
        "\n",
        "# Specify model\n",
        "model = Model(inputs=conv_base.input, outputs=preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jBi1s-KbE2"
      },
      "source": [
        "Note that the weights of the new dense layers are initialized with random values. So we need to train the dense layers on our dataset to make it work.\n",
        "\n",
        "### Questions 9.2\n",
        "1. What should N be in the above code block?\n",
        "2. Re-run the code block with the correct N.\n",
        "3. What does GlobalAveragePooling2D do?\n",
        "\n",
        "Hint: You can print all layers and print properties like name, type and input shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B36lhXo7E5-6"
      },
      "outputs": [],
      "source": [
        "for i,layer in enumerate(model.layers):\n",
        "  layer_name = layer.name\n",
        "  layer_type = layer.__class__.__name__\n",
        "\n",
        "  try:\n",
        "    input_shape = layer.input.shape\n",
        "    print(f\"Layer {i} has name {layer_name} and type {layer_type}, and its input shape is {input_shape}\")\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "# Or use the summary function:\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO_YKGvp20I2"
      },
      "source": [
        "We will only be training the new dense layers that we added. Disable training for all previous layers and enable for new layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL_pekjgUGnz"
      },
      "outputs": [],
      "source": [
        "total_num_layers = len(model.layers)\n",
        "num_base_layers = len(conv_base.layers)\n",
        "print(f\"Total number of layers is {total_num_layers}\")\n",
        "print(f\"Number of pretrained base layers is {num_base_layers}\")\n",
        "\n",
        "for layer in model.layers[:num_base_layers]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[num_base_layers:]:\n",
        "    layer.trainable=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1acWCuVD3xvI"
      },
      "source": [
        "We are now ready to start training the model using\n",
        "- Adam optimizer\n",
        "- loss function will be categorical cross entropy\n",
        "- evaluation metric will be accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qXOMhrGM36f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Set up optimizer\n",
        "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=100,\n",
        "    decay_rate=1e-6)\n",
        "sgd_optimizer = optimizers.SGD(learning_rate=lr_schedule,momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile model - make it trainable\n",
        "model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "step_size_train = train_generator.n//train_generator.batch_size # Number of mini-batches per epoch (training)\n",
        "step_size_val = validation_generator.n//validation_generator.batch_size # Number of mini-batches per epoch (validation)\n",
        "\n",
        "# Train model for 10 epochs\n",
        "history = model.fit_generator(train_generator,\n",
        "                   validation_data=validation_generator,\n",
        "                   validation_steps=step_size_val,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdAelWXnO_tG"
      },
      "source": [
        "### Questions 9.3\n",
        "Look at the outputs of the training.\n",
        "\n",
        "1. What is the difference between 'loss' and 'val_loss'?\n",
        "2. What is the difference between 'accuracy' and 'val_accuracy'?\n",
        "3. Do they behave the same, or do they behave differently? Try to explain what you see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhEq1iiIcYu"
      },
      "source": [
        "We can plot the accuracy as a function of the epoch number (and epoch is when all training have been processe exactly once). Notice how the training accuracy increases in the beginning and then stagnates. This is normal training behavior. The validation accuracy stays farily constant, meaning that training the model further (for more epochs) would most likely *not* improve the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiLFHoLZJDzH"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdl_4ecOJdXW"
      },
      "source": [
        "For the loss, we observe a similar pattern, except that the loss increases over time (which it should)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or5w8xQgJngo"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBQU1wAPD4-w"
      },
      "source": [
        "## Task 10: Deploying the model\n",
        "Here is how to deploy the model and integrate with OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO1p9_2LWUMH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Pick first image of first class\n",
        "filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n",
        "img_path = str(filelist[1])\n",
        "print(f\"File path: {img_path}\")\n",
        "\n",
        "# Remember to convert to RGB\n",
        "img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img);\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZMwJJ1w_YVW"
      },
      "source": [
        "Now, make sure that the image shape and the pixel intensity range is as expected by the network (required shape = `1x120x120x3` and intensity range from -1 to 1):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THYIJ5n2fX_u"
      },
      "outputs": [],
      "source": [
        "img = cv2.resize(img, (120, 120))\n",
        "img = (img[...,::-1].astype(np.float32))\n",
        "img /= 127.5\n",
        "img -= 1.\n",
        "img = np.expand_dims(img,0)\n",
        "print(img.shape,img.min(),img.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52pNra-fRXRk"
      },
      "source": [
        "Run the image through the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jeg17ghAg5H4"
      },
      "outputs": [],
      "source": [
        "class_probabilities = model.predict(img)[0]\n",
        "print(class_probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsU3tcsOJaz1"
      },
      "outputs": [],
      "source": [
        "# pick class with highest probability\n",
        "class_index = (-class_probabilities).argsort()[0]\n",
        "print(f'The image displays a {classes[class_index]}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CLHhqpL_n8M"
      },
      "source": [
        "## Exporting to TensorFlow JS and hosting a web service on GitHub\n",
        "If you wanted to, you could in principle deploy your model and make a nice web service like this one:\n",
        "https://klaverhenrik.github.io/transferlearning/. The webpage is hosted on GitHub using [GitHub Pages](https://pages.github.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwL76U482A9A"
      },
      "source": [
        "## Task 11: Image retrieval (optional task)\n",
        "Implement an image search engine (this is also called image retrieval). Use MobileNet features, and if you have time do a comparison with raw pixel features.\n",
        "\n",
        "The search engine could work like this:\n",
        "\n",
        "1. Given an input image from the validation set, pre-process it and feed it through MobileNet to calculate the feature vector.\n",
        "2. Then perform a K-NN search with K=10 against the feature vectors in the training set.\n",
        "3. Then return the corresponding 10 closest images.\n",
        "\n",
        "In principle we should preprocess the input image like this:\n",
        "```\n",
        "img = image.load_img(some_image_path, target_size=(120, 120))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0)\n",
        "img_preprocessed = preprocess_input(img_data)\n",
        "```\n",
        "But it is okay to cheat and use the image data that have already been preprocessed and run through MobileNet.\n",
        "\n",
        "**Question:** What kind of search results do you expect to see when using MobileNet features vs.**bold text** using raw pixel features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbnTvMij-Pt9"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgy3bOwKwFrA"
      },
      "source": [
        "## Task 12: K-means clustering (optional task)\n",
        "As stated above, the underlying hypothesis of using K-NN to classifiy images using MobileNet features is that *objects that are similar will map to the approximate same location on some manifold.*\n",
        "\n",
        "Here we will perform K-means clustering and verify that this is in fact the case. For the record, recall that the K-means method is an *unsupervised learning method*, so it doesn't know anything about the class labels.\n",
        "\n",
        "**Your task** is to perform K-means clustering twice on your training dataset: first using the raw intensity features (``train_features_raw``), then using the MobileNet features (``train_features_mobilenet``). Use as many clusters as you have classes, which is 3 if you also used cat, dog, and horse.\n",
        "\n",
        "For each cluster, print the class labels (cat, dog, horse) or the class indices (0, 1, 2) of all images in that cluster. Explain what you observe and compare between MobileNet features and raw pixel intensities.\n",
        "\n",
        "**Note** that we wont be needing the validation set in this task. Why? Because K-means is an unsupervised learning method often used to perform exploratory data analysis. Here we are pretending that we dont know the true labels. We are just interested in seeing if the image data form clusters or not.\n",
        "\n",
        "Again, **you don't have to implement K-means clustering from scratch**. You can use scikit-learn (and Google).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsib49ZtWguk"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmo4-o-Yoxrz"
      },
      "source": [
        "## Task 13: Visualising high-dimensional datasets using PCA and t-SNE in Python (optional task)\n",
        "Simply speaking, t-SNE is a dimensionality reduction technique that maps N-dimensionalal data to, say, 2D, where points that are close in N-dimensional space are close in the 2D space.\n",
        "\n",
        "Thus, we can use t-SNE to visualize our high-dimensional image dataset in 2D. If we colorize the data points according to their class label, we can see if our choice of feature representation (i.e., raw pixel values or MobileNet features) is useful for separating the classes.\n",
        "\n",
        "So lets do that for both choices of features.\n",
        "\n",
        "Read more here: https://builtin.com/data-science/tsne-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p60ymoyU9Rl"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "def run_tsne(features,labels,classes=classes):\n",
        "  X = features\n",
        "  y = labels\n",
        "\n",
        "  feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
        "  df = pd.DataFrame(X,columns=feat_cols)\n",
        "  df['y'] = y\n",
        "  df['label'] = df['y'].apply(lambda i: str(i))\n",
        "  X, y = None, None\n",
        "  print('Size of the dataframe: {}'.format(df.shape))\n",
        "\n",
        "  rndperm = np.random.permutation(df.shape[0])\n",
        "\n",
        "  N = 10000\n",
        "  df_subset = df.loc[rndperm[:N],:].copy()\n",
        "  data_subset = df_subset[feat_cols].values\n",
        "  pca = PCA(n_components=3)\n",
        "  pca_result = pca.fit_transform(data_subset)\n",
        "  df_subset['pca-one'] = pca_result[:,0]\n",
        "  df_subset['pca-two'] = pca_result[:,1]\n",
        "  df_subset['pca-three'] = pca_result[:,2]\n",
        "  print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "  time_start = time.time()\n",
        "  tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "  tsne_results = tsne.fit_transform(data_subset)\n",
        "  print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "  df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
        "  df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
        "  plt.figure(figsize=(8,8))\n",
        "  sns.scatterplot(\n",
        "      x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "      hue=\"y\",\n",
        "      palette=sns.color_palette(n_colors=len(classes)),\n",
        "      data=df_subset,\n",
        "      legend=\"full\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q67GcA6GVhso"
      },
      "source": [
        "t-SNE allows us to visually verify that when using the raw pixels as features, the data points do not form clusters corresponding to the classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8A_28_Lo3I-"
      },
      "outputs": [],
      "source": [
        "run_tsne(train_features_raw,train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoB_3Afo8vP"
      },
      "source": [
        "Figure above: Raw pixels mapped to 2D using t-SNE. The data points do not form clusters corresponding to the classes (blue, orange, green dots).\n",
        "\n",
        "Now, let's repeat the t-SNE analysis, but this time using the MobileNet features is input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLrTSGYWo-HP"
      },
      "outputs": [],
      "source": [
        "run_tsne(train_features_mobilenet,train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KocTx0l_pILi"
      },
      "source": [
        "Figure above: Now, we see at least some clustering corresponding to the classes.\n",
        "\n",
        "This - again - tells us that the MobileNet (or CNN) features are better at separatin the classes than the raw pixel features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 14: Built your own dataset using DuckDuckGo (optional)\n",
        "You can use DuckDuckGo to download images of categories of your choice.\n",
        "\n",
        "To install the [`ddgs`package](https://github.com/deedy5/ddgs) run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ddgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make a search for \"cat\", run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ddgs import DDGS\n",
        "with DDGS() as ddgs:\n",
        "  results = ddgs.images('cat', max_results=1)\n",
        "results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To download the image from the URL, run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "r = requests.get(results[0]['image'])\n",
        "with open('cat.jpg', 'wb') as f:\n",
        "  f.write(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A few things to consider\n",
        "- How do you organize the data on disk?\n",
        "- After downloading data based on a URL, how do you make sure that the downloaded file is actually a loadable image?\n",
        "- How do you assess the quality of your dataset? (E.g., are there any mislabelled in there? If so, what would you do about it?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtSiBK3DURiC"
      },
      "source": [
        "## Ideas for further work\n",
        "1. In the above example we have not optimized the pre-trained weights of the convolutional base (i.e., the encoder). To improve performance further you could enable training in all layers (including the convolutoinal base) and re-train the network. This is called *fine-tuning*.\n",
        "2. Another way to improve model performance is by *data augmentation*. Have a look [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to see what kind of augmentation is possible. Why do you think data augmentation helps improve the performance of your model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
